创建于 2022-01-05
关键词: 机器学习, 降维, 主成分分析.

## 主成分分析概要

一般情况下数据集特征维数会特别大，在用机器学习算法建模的时候由于维数灾难所带来的各种问题经常需要把数据降到一个较低的维度，同时又要保证丢失的信息尽可能的少，如果丢失的信息量在可忍受的范围内可以降低到三维或二维则可以很容易把样本点的分布在二维或三维的直角坐标系中展示出来，这样可视化样本点分布对数据的理解有很大的帮助。而主成分分析(Principal Components Analysis, PCA)就是达到降维目的的降维算法之一。

主成分分析算法的输入就是原始数据集的整个特征空间的数据，输出的每一个主成分都是其所有输入特征的线性映射，并且所有主成分两两之间相关性为$0$，而且所有主成分根据方差的不同有一个排序，方差最大的称为第一主成分，其次称为第二主成分，以此类推。一般来说随着数据维度的减小，数据信息会有所损失，主成分分析就是为了尽可能在信息损失最少的情况下对数据集进行降维，具体的降维操作其实就是把排名靠后的若干个主成分直接去掉，也就是说如果经过主成分分析算法变换后保留所有主成分，则不会有信息损失，但是这样维数就没有发生变化。主成分分析在特征工程中属于特征提取操作，其本质是把一个特征空间映射到了另外一个特征空间。

## 主成分分析数学原理

主成分分析的本质就是先把在原始坐标空间的样本点映射到另外一个维数相同坐标空间，然后再把新空间里不要的维度删掉，即把新空间里的样本点中不要的维度的坐标直接置$0$，由于所有样本在不要的维度里的坐标都为$0$，所以也没必要表示出这些不要的维度。

想象一种简单的情况，假设要把$3$维空间中的坐标点降维到$2$维，只需要在这个$3$维空间中找到一个平面，然后把所有样本点映射到这个平面上即可，同理推广到高维，假设共有$d$维特征，如果想降维到$d'$维，则只需要在这个$d$维空间中找到一个$d'$维超平面，然后把所有样本点映射到这个$d'$维超平面上即可。

为了简单起见，主成分分析中映射方式选择简单易于理解的投影，接着就是考虑如何确定合适的超平面，有趣的是基于两种思想推导出来的结果是一致的。一种思想是最近重构性，即样本点总体到这个超平面的距离足够近，距离足够近重构回原高维样本误差就越小；另一种是最大可分性，就是投影到这个超平面的样本点尽可能的分开，也就是使得各个方向的方差尽可能的大。

以下首先基于最近重构性思想进行推导。

假设在原始坐标空间的一个样本矢量为$\boldsymbol{x}$，这个样本映射到另外一个坐标空间的样本矢量为$\boldsymbol{x}'$，如果新坐标的原点在原始坐标空间中对应的矢量为$\boldsymbol{\mu}$，显然有
$$
\boldsymbol{x} = \boldsymbol{\mu} + \boldsymbol{x}'
$$
分别将矢量$\boldsymbol{x}$和$\boldsymbol{x}'$写成两个坐标系下的坐标形式：
$$
\begin{aligned}
\boldsymbol{x} &= \left(x_1,\cdots,x_d\right)^{\mathrm T} \\
\boldsymbol{x}' &= \left( a_1,\cdots,a_d\right)^{\mathrm T}
\end{aligned}
$$
则有
$$
\boldsymbol{x} = \boldsymbol\mu + \sum_{i=1}^{d}a_i\boldsymbol{e}_i
$$
其中$\boldsymbol{e}_i$为新坐标系下的单位基矢量。对上式进行变换推导可得：
$$
\begin{aligned}
\sum_{i=1}^{d}a_i\boldsymbol{e}_i &= \boldsymbol{x} - \boldsymbol\mu \\
\boldsymbol{e}_i^{\mathrm T}\sum_{i=1}^{d}a_i\boldsymbol{e}_i &= \boldsymbol{e}_i^{\mathrm T}\left(\boldsymbol{x} - \boldsymbol\mu\right) \\
\boldsymbol{e}_i^{\mathrm T}a_i\boldsymbol{e}_i &= \boldsymbol{e}_i^{\mathrm T}\left(\boldsymbol{x} - \boldsymbol\mu\right) \\
a_i &= \boldsymbol{e}_i^{\mathrm T}\left(\boldsymbol{x} - \boldsymbol\mu\right)
\end{aligned}
$$
然而如果只保留新坐标系下$d'<d$个元素，然后用保留的$d'$个元素来恢复原坐标系下的$d$维矢量：
$$
\hat{\boldsymbol{x}} = \boldsymbol\mu + \sum_{i=1}^{d'}a_i\boldsymbol{e}_i
$$
显然$\hat{\boldsymbol{x}}$只是对$\boldsymbol{x}$的近似，用$\hat{\boldsymbol{x}}$来代替$\boldsymbol{x}$就会出现一定的误差。原则上$\boldsymbol{\mu}$可以是任意矢量，但是为了后续推导方便，在主成分分析算法中$\boldsymbol{\mu}$为所有样本的均值矢量，由于$\boldsymbol{\mu}$是均值矢量在后续的推导中就会由此而产生协方差矩阵，从而可以利用协方差矩阵的一些特性使得推导变得更加简单。

然后寻找一组最优的基矢量$\{\boldsymbol{e}_i,\cdots,\boldsymbol{e}_d\}$，使得在只保留$d'$个元素的条件下，由新的坐标恢复样本集$D$的均方误差最小，即求解如下的优化问题：
$$
\min\frac{1}{n}\sum_{k=1}^n\Vert\boldsymbol{x}_k-\hat{\boldsymbol{x}}_k\Vert^2
$$
如果用$a_{ki}$表示第$k$个样本在新坐标系下的第$i$维特征，则可以得到
$$
\begin{aligned}
\boldsymbol{x}_k - \hat{\boldsymbol{x}}_k &= \left(\boldsymbol\mu+\sum_{i=1}^{d}a_{ki}\boldsymbol{e}_i\right) - \left(\boldsymbol\mu+\sum_{i=1}^{d'}a_{ki}\boldsymbol{e}_i\right) \\
&= \sum_{i=d'+1}^da_{ki}\boldsymbol{e}_i
\end{aligned}
$$
所以
$$
\begin{aligned}
\frac{1}{n}\sum_{k=1}^n\Vert\boldsymbol{x}_k-\hat{\boldsymbol{x}}_k\Vert^2 &= \frac{1}{n}\sum_{k=1}^n\Vert\sum_{i=d'+1}^da_{ki}\boldsymbol{e}_i\Vert^2 \\
&= \frac{1}{n}\sum_{k=1}^n\left(\sum_{i=d'+1}^da_{ki}\boldsymbol{e}_i\right)^{\mathrm T}\left(\sum_{i=d'+1}^da_{ki}\boldsymbol{e}_i\right) \\
&= \frac{1}{n}\sum_{k=1}^n\sum_{i=d'+1}^da_{ki}^2 \\
&= \frac{1}{n}\sum_{k=1}^n\sum_{i=d'+1}^d\left(\boldsymbol{e}_i^{\mathrm T}\left(\boldsymbol{x}_k-\boldsymbol\mu\right)\right)^2 \\
&= \frac{1}{n}\sum_{k=1}^n\sum_{i=d'+1}^d\boldsymbol{e}_i^{\mathrm T}\left(\boldsymbol{x}_k-\boldsymbol\mu\right)\left[\boldsymbol{e}_i^{\mathrm T}\left(\boldsymbol{x}_k-\boldsymbol\mu\right)\right]^{\mathrm T} \\
&= \frac{1}{n}\sum_{k=1}^n\sum_{i=d'+1}^d\boldsymbol{e}_i^{\mathrm T}\left(\boldsymbol{x}_k-\boldsymbol\mu\right)\left(\boldsymbol{x}_k-\boldsymbol\mu\right)^{\mathrm T}\boldsymbol{e}_i \\
&= \sum_{i=d'+1}^{d}\boldsymbol{e}_i^{\mathrm T}\left[\frac{1}{n}\sum_{k=1}^n\left(\boldsymbol{x}_k-\boldsymbol\mu\right)\left(\boldsymbol{x}_k-\boldsymbol\mu\right)^{\mathrm T}\right]\boldsymbol{e}_i
\end{aligned}
$$
如果定义矩阵：
$$
\mathbf\Sigma = \frac{1}{n}\sum_{k=1}^{n}\left(\boldsymbol{x}_k-\boldsymbol\mu\right)\left(\boldsymbol{x}_k-\boldsymbol\mu\right)^{\mathrm T}
$$
则$\mathbf\Sigma$恰好是样本集$D$的协方差矩阵，则优化问题变为
$$
\min\sum_{i=d'+1}^{d}\boldsymbol{e}_i^{\mathrm T}\mathbf\Sigma\boldsymbol{e}_i
$$
可以证明，协方差矩阵一定是半正定矩阵，而半正定矩阵$\mathbf A$对于任何非零向量$\boldsymbol{x}$都有$\boldsymbol{x}^{\mathrm T}\mathbf A\boldsymbol{x}\geq0$，因此可以知道当$\boldsymbol{e}_i$为零矢量的时候$\sum_{i=d'+1}^d\boldsymbol{e}_i^{\mathrm T}\mathbf\Sigma\boldsymbol{e}_i$可以取得最小值$0$，但是这里的$\boldsymbol{e}_i$为单位正交基，也就是说这是一个约束优化问题，约束为
$$
\begin{cases}
\Vert \boldsymbol{e}_i\Vert^2 = 1, &i=1,\cdots,d \\
\boldsymbol{e}_i\boldsymbol{e}_j=0, &i,j=1,\cdots,d \text{ and } i\neq{j} 
\end{cases}
$$
因此可以构造拉格朗日函数
$$
L = \sum_{i=d'+1}^d\boldsymbol{e}_i^{\mathrm T}\mathbf\Sigma\boldsymbol{e}_i - \sum_{i=d'+1}^d\lambda_i\left(\boldsymbol{e}_i^{\mathrm T}\boldsymbol{e}_i-1\right)
$$
对每一个基矢量求偏导数：
$$
\begin{aligned}
\frac{\partial}{\partial\boldsymbol{e}_j}L &= \frac{\partial}{\partial\boldsymbol{e}_j}\left(\sum_{i=d'+1}^{d}\boldsymbol{e}_i^{\mathrm T}\mathbf\Sigma\boldsymbol{e}_i\right) - \frac{\partial}{\partial\boldsymbol{e}_j}\left(\sum_{i=d'+1}^d\lambda_i\left(\boldsymbol{e}_i^{\mathrm T}\boldsymbol{e}_i-1\right)\right) \\
&=\frac{\partial}{\partial\boldsymbol{e}_j}\left(\boldsymbol{e}_j^{\mathrm T}\mathbf\Sigma\boldsymbol{e}_j\right) - \frac{\partial}{\partial\boldsymbol{e}_j}\left(\lambda_j\left(\boldsymbol{e}_j^{\mathrm T}\boldsymbol{e}_j-1\right)\right) \\
&= \left(\mathbf\Sigma+\mathbf\Sigma^{\mathrm T}\right)\boldsymbol{e}_j - \lambda_j\left(\mathbf I+\mathbf I^{\mathrm T}\right)\boldsymbol{e}_j \\
&= 2\mathbf\Sigma\boldsymbol{e}_j - 2\lambda_j\boldsymbol{e}_j
\end{aligned}
$$
其中$j=d'+1,\cdots,d$ 。

以上推导过程应用了如下2个结论：
1、协方差矩阵和单位矩阵都是对称矩阵，因此$\mathbf\Sigma=\mathbf\Sigma^{\mathrm T}$，$\mathbf I=\mathbf I^{\mathrm T}$；
2、$\frac{\partial}{\partial\boldsymbol{x}}\boldsymbol{x}^{\mathrm T}\mathbf B\boldsymbol{x}=\left(\mathbf B+\mathbf B^{\mathrm T}\right)\boldsymbol{x}$。

令每一个基矢量的偏导数等于$\boldsymbol0$，即
$$
2\mathbf\Sigma\boldsymbol{e}_j - 2\lambda_j\boldsymbol{e}_j = \boldsymbol 0
$$
结合约束条件最终可得优化问题的解因满足：
$$
\begin{cases}
\mathbf\Sigma\boldsymbol{e}_j = \lambda_j\boldsymbol{e}_j, &j=d'+1,\cdots,d \\
\Vert \boldsymbol{e}_i\Vert^2 = 1, &i=1,\cdots,d \\
\boldsymbol{e}_i\boldsymbol{e}_j=0, &i,j=1,\cdots,d \text{ and } i\neq{j}
\end{cases}
$$
显然，使得上式成立的$\lambda_j$和$\boldsymbol{e}_j$分别为矩阵$\mathbf \Sigma$的特征值和特征矢量，由于协方差矩阵$\mathbf  \Sigma$是$d$阶方阵，因此有$d$个特征值和特征矢量，因此需要进一步找出$d-d'$个特征矢量以满足最终优化问题的解。

因此原优化问题可以进一步表示为
$$
\begin{aligned}
\min\sum_{i=d'+1}^{d}\boldsymbol{e}_i^{\mathrm T}\mathbf\Sigma\boldsymbol{e}_i &= \min\sum_{i=d'+1}^d\boldsymbol{e}_i^{\mathrm T}\lambda_i\boldsymbol{e}_i \\
&= \min\sum_{i=d'+1}^d\lambda_i
\end{aligned}
$$
由于协方差矩阵$\mathbf \Sigma$为对称矩阵，因此它的$d$个特征值均为实数，并且$d$个特征矢量的每个元素也都是实数，不会出现复矢量的情况，所以上式表达的意思是选取$d-d'$个最小特征值的累加和就是最终的优化结果，而剩下的$d'$个特征值所对应的特征矢量就是降维到$d'$维所需要的特征矢量。

另外值得一提的是，由于协方差矩阵$\mathbf \Sigma$是实对称矩阵，因此它的所有不同特征值对应的特征矢量都是两两正交的，所以如果所有特征值都不一样约束条件里的正交条件就无需使用，如果特征方程存在重根，即存在相同的特征值，那么这些特征值对应的特征矢量是线性无关的，但不一定正交，为了满足正交的约束条件，可以通过正交化处理得到最终的单位正交基。

最终可以得到这样的结论：如果希望将一个样本集$D$中的$d$维特征矢量在一个新的坐标系下只用$d'$个特征表示，那么可以将新坐标系的原点放在$D$的样本矢量的均值矢量$\boldsymbol\mu$的位置，而以$D$的协方差矩阵的最大的$d'$个特征值所对应的特征矢量作为基矢量，这样可以保证只用保留的$d'$维特征恢复原矢量时均方误差最小。

以上是基于最近重构性思想得出的推导，以下给出基于最大可分性思想的推导。

仍然假设映射到新坐标空间中的单位正交基为$\boldsymbol{e}_1,\boldsymbol{e}_2,\cdots,\boldsymbol{e}_d$，均值矢量为$\boldsymbol{\mu}$，假设要降维到$d'$维，则可令$\mathbf E=\left(\boldsymbol{e}_1,\cdots,\boldsymbol{e}_{d'}\right)$，则原始样本点$\boldsymbol{x}_k$在新空间中超平面上的投影是$\mathbf E^{\mathrm T}\left(\boldsymbol{x}_k-\boldsymbol\mu\right)$，要让所有样本点的投影尽可能分开，则应该尽可能让投影的每一个维度的方差最大化，也即在新空间的$d$维特征中找到$d'$个维度使得这$d'$个维度的方差和最大，于是优化目标可写为
$$
\max_{\mathbf E} \left(\frac{1}{n}\operatorname{tr}\sum_{k=1}^n\mathbf E^{\mathrm T}\left(\boldsymbol{x}_k-\boldsymbol{\mu}\right)
\left(\boldsymbol{x}_k-\boldsymbol\mu\right)^{\mathrm T}\mathbf E\right)
$$
同样，这也是约束优化问题，约束为
$$
\mathbf E^{\mathrm T}\mathbf E=\mathbf I
$$
假设经过中心化处理后的样本为$\boldsymbol x_k'$，则有
$$
\boldsymbol{x}_k-\boldsymbol\mu=\boldsymbol x_k'
$$
于是优化目标可以化简为
$$
\max_{\mathbf E} \left(\frac{1}{n}\operatorname{tr}\sum_{k=1}^n\mathbf E^{\mathrm T}\boldsymbol{x}_k'\boldsymbol{x}_k'^{\mathrm T}\mathbf E\right)
$$




## 参考文献

[1] 周志华. 机器学习[M]. 清华大学出版社, 2016.
[2] 刘家锋, 刘鹏, 张英涛等. 模式识别[M]. 哈尔滨工业大学出版社, 2017.



